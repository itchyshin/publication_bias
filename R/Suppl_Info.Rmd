---
title: Methods for testing publication bias in ecological and evolutionary meta-analyses
author: Shinichi Nakagawa, Malgorzata Lagisz, Michael D. Jennions, Julia Koricheva, Daniel W.A. Noble, Timothy H. Parker, Alfredo Sanchez-Tojar, Yefeng Yang, Rose O'Dea
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    code_download: yes
    code_folding: hide
    depth: 5
    toc: yes
    toc_float: yes
    toc_depth: 5
  pdf_document:
    toc: yes
subtitle: Supplementary Information - Appendix S1-S5 including figure code
bibliography: references.bib
biblio-style: "apalike"
csl: jane.csl
link-citations: yes
---

```{r setup, include = FALSE}
# knitr setting
knitr::opts_chunk$set(
  #message = FALSE,
  warning = FALSE, # no warnings
  tidy = TRUE,
  cache = TRUE
)

# cleaning up
rm(list = ls())

## numbers >= 10^5 will be denoted in scientific notation, and rounded to 2 digits
options(digits=3)
# clearing up
```

```{r, eval =FALSE}
# install the orchaRd package
install.packages("devtools")
install.packages("tidyverse")
install.packages("metafor")
install.packages("patchwork")
install.packages("R.rsp")

devtools::install_github("itchyshin/orchard_plot", subdir = "orchaRd", force = TRUE, build_vignettes = TRUE)

# install the dmetar pacakge
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("MathiasHarrer/dmetar")
```


```{r function, echo = FALSE, eval = TRUE}
#TODO - Alfredo - this is good but it is in the author contribution - can we delete this?? - otherwise, we need to do this section by section (or please rearrange this if you still want it)
###################################################################
# Worked example publication bias methodology
#
# Code written by:
#
#         Alfredo Sanchez-Tojar (alfredo.tojar@gmail.com); Departament of Evolutionary Biology, Bielefeld University, Germany
#
#         Yefeng...
#
# Code validated by:
#
#         Shinichi Nakagawa (); ...
#
# Script first created on November 20th, 2020

###################################################################
# Description of script and Instructions
###################################################################

# This script is to generate worked examples to show how to test for
# publication bias when several layers of non-independence exist in
# the data, which is common for meta-analytic datasets in ecology 
# and evolution. We provide two worked examples, one for correlation
# effect sizes (r) and one for mean comparisons (SMD and lnRR).


# requried packages
pacman::p_load(tidyverse,
               metafor,
               gt,
               pander,
               stringr,
               openxlsx,
               rotl,
               ape,
               kableExtra,
               patchwork,
               here,
               lme4,
               readxl,
               orchaRd,
               patchwork,
               cowplot,
               metaviz,
               meta,
               ggplotify,
               grid,
               dmetar,
               viridis
               )

# necessary custom functions

# extracting mean and CI from each metafor model
estimates.CI <- function(model){
  db.mf <- data.frame(model$b,row.names = 1:nrow(model$b))
  db.mf <- cbind(db.mf,model$ci.lb,model$ci.ub,row.names(model$b))
  names(db.mf) <- c("mean","lower","upper","estimate")
  return(db.mf[,c("estimate","mean","lower","upper")])
}

# r and Zr transfromation can be don eby tanh and atanh - base function

```

## Appendix S1: Survey of publication bias tests reported in ecology and evolution meta-analyses

*Ask Rose to finish this off* - references are not done etc - need fixing - use  (ask Yefeng and Alfredo do help as well) -- e.g. [@viechtbauer2010conducting]

### S1.1: Survey method

The main objective of the literature survey was to capture the types of publication bias methods recently used in the field of ecology and evolution. The survey reported here was conducted alongside a larger survey on reporting standards from the PRISMA-EcoEvo project (Preferred Reporting Items for Systematic reviews and Meta-Analyses in Ecology and Evolutionary Biology; full details and materials provided in {O’Dea, 2019 #28}). The survey was based on 102 meta-analysis papers published between 1 January 2010 and 25 March 2019. We aimed for the sample of 102 meta-analyses to be representative of recently published meta-analyses in ecology and evolutionary biology journals.

#### S1.1.1 Representative Sample

To select the representative sample of meta-analysis publications, we first searched the Scopus database for papers with (“meta-analy\*” OR “metaanaly\*” OR “meta-regression”) in the title, abstract, or keywords (where the asterisk allows for any ending to the word, such as “meta-analyses” or “meta-analysis”), restricted the date to papers published after 2009, and restricted the ISSN (International Standard Serial Number) to journals classified as ‘Ecology’ and/or ‘Evolutionary Biology’ by the 2017 rankings of the ISI InCites Journal Citation Reports. On 25 March 2019 this search returned 1,668 papers from 134 journals.

Second, the returned papers were categorised as being from journals listed as ‘Ecology’, ‘Evolution’, or ‘Both’, and we reduced the number of journals and papers. To reduce the number of journals we arranged journals in descending order of frequency (i.e. to indicate which journals published the most meta-analyses). After removing journals in more applied sub-fields (such as ecology economics), we retained the top 10 journals classified as ecology, the top 10 journals classified as evolutionary biology, and the top 11 journals classified as both ecology and evolutionary biology. The list of 31 retained journals is shown in Table S1. Next, we selected 297 studies to be screened for inclusion in the sample, by using the ‘sample’ function in R (v. 3.5.1; R Core Team, 2018) to randomly select up to 17 studies from the journals classified as ‘Evolutionary Biology’ (57 studies total) or ‘Both’ (150 studies total), and up to 9 studies from journals classified as ‘Ecology’ (90 studies total). 

##### Table S1.1
Journals screened in our search for a representative sample of meta-analyses published in ecology and evolutionary biology. ISI Classification is based on the 2017 ISI InCites Journal Citation Reports. The number of papers returned from the Scopus database search is described by ‘N hits’, while the number of studies selected for screening is described by ‘N screened’.
```{r}
# getting the data and formatting some variables (turning character vectors to factors)
read_excel(here("data/TableS1.xlsx"), na = "NA") %>% 
   #mutate_if(is.character, as.factor) %>%  
  kbl() %>%
  kable_paper() %>% 
  kable_styling("striped", position = "left")
```

#### S1.1.2 Inclusion criteria and screening methods
The sample of 102 meta-analysis papers met the following four inclusion criteria: (1) the study addressed a question in the fields of ecology and evolutionary biology; (2) claimed to present results from a meta-analysis; (3) performed a search for, and collected, data from the primary literature; (4) used a statistical model to analyse effect sizes that were collected from multiple studies. Paper screening was conducted in two stages. First, two authors (RO and ML) conducted parallel abstract screening. Conflicting decisions were discussed and resolved. Second, 64% of screened abstracts underwent parallel full-text screening by RO and ML, in consultation with SN.


#### S1.1.3 Assessing 102 meta-analysis papers
Papers were independently assessed by seven authors (RO, DN, JK, MJ, ML, RO, SN, and TP) as part of a larger, time-consuming survey. The one survey question pertaining to this project was: “Which publication bias tests are reported in the paper? (Select all that apply)”. There were 11 possible choices for respondents to select: (A) Funnel plots (including contour-enhanced funnel plots); (B) Normal quantile (QQ) plots (Wang & Bushman); (C) Correlation-based tests (e.g. Begg & Manzumdar rank correlation); (D) Regression-based tests (e.g. Egger regression and its variants); (E) File drawer numbers or fail-safe N (Rosenthal, Orwin or Rosenberg method); (F) Trim-and-fill tests; (G) P-curve, P-uniform or its variants; (H) Selection (method) models (e.g. Copas, Hedges or lyengar & Greenhouse model); (I) Time-lag bias tests (e.g., regression or correlation on the relationship between effect sizes and time or cumulative meta-analysis); (J) None reported and (K) ‘Other’ methods. According to Sutton ({, 2009 #17}) (see also {Vevea, 2019 #16}), Methods A-D are tests detecting publication bias, whereas Methods E-F are assessing the impact of publication bias.

### S1.2 Results
Among the 102 assessed papers, 17.8% did not report any tests of publication bias. 
Most meta-analysis papers reported one or more tests of publication bias (17.8% of assessed papers did not include any assessment of publication bias). These results suggest tests of publication bias have become more common in recent year in ecology and evolutionary biology, as over half of older meta-analyses assessed by Nakagawa and Santos ({, 2012 #19}) and Koricheva and Gurevitch ({Koricheva, 2014 #18}) did not report any tests of publication bias (although our results are not directly comparable, due to different survey methods). Still, inferential tests of publication bias remain uncommon. By far the most popular test of publication bias were funnel plots (32.4%; Table S2), with all remaining methods represented by fewer than 15% of papers. All methods except ‘selection models’ were present in at least one paper (with ‘other’ being selected for a weighted histogram used by {Loydi, 2013 #70}; Table S2). The absence of selection model methods could be because these methods are comparatively technically challenging, or because ecologists and evolutionary biologists are not yet aware of the benefits of these methods.

##### Table S1.2
Frequency with which publication bias tests were reported in the 102 meta-analysis publications, ranked in order of decreasing popularity. No tests were reported for 17.80% of papers.

```{r}
# getting the data and formatting some variables (turning character vectors to factors)
read_excel(here("data/TableS2.xlsx"), na = "NA") %>% 
   #mutate_if(is.character, as.factor) %>%  
  kbl() %>%
  kable_paper() %>% 
  kable_styling("striped", position = "left")
```

## Appendix S2: Equivalence between Equations 8 and 9

As in the main text, Equation 8 is as follows
$$
z_{i} = \beta_{0} + \beta_{1}prec_i + e_i,\\
$$
where $e_{i} \sim \mathcal{N}(0, \sigma_e^2)$. We can rewrite this as:

$$
y_{i}/se_i = \beta_{0} + \beta_{1}(1/se_i) + e_i,\\
$$
If we multiple both side by $se_i$, we have:

$$
y_{i} = \beta_{0}se_i + \beta_{1} + e_{i}se_{i},
$$

which is basically the same, if $\beta_0$ and $\beta_1$ are swapped, as:

$$
y_{i} = \beta_{0} + \beta_{1}se_i + \epsilon_i,
$$
where $\epsilon_i \sim \mathcal{N}(0, v_i\phi)$ as in Equation 9. We can show this using a simulated data as well (we use the data used for Figure 3 and 4). Below the first result is from Equation 8 and the second Equation 9:


```{r}
# creating data - the same data as Figures 3 + 4
set.seed(77777)
# setting parameters
n.effect <- 100
sigma2.s <- 0.05
beta1 <- 0.2

# using negative binomial we get good spread of sample size
n.sample <- rnbinom(n.effect, mu = 30, size = 0.7) + 4

# variance for Zr
vi <- 1/(n.sample - 3)

# moderator x 1
xi <- rnorm(n.effect)

# there is underling overall effect to r = 0.2 or Zr = 0.203
Zr <- atanh(0.2) + beta1*xi + rnorm(n.effect, 0, sqrt(sigma2.s)) + rnorm(n.effect, 0, sqrt(vi))
#qplot(Zr, 1/sqrt(vi))

# data frame
dat <- data.frame(yi = Zr, vi = vi, sei = sqrt(vi), xi = xi, ni = n.sample, prec = 1 / sqrt(vi), wi = 1/vi, zi = Zr/sqrt(vi))
rows <- 1:nrow(dat)
expected <- which(1/dat$sei < 5 & dat$yi < 0.25)
unexpected <- which(1/dat$sei > 4.7 & dat$yi > 0.25)
col_point <- ifelse(rows %in% expected, "red", ifelse(rows %in% unexpected, "blue", "black"))
dat$col_point <- col_point

# data with "publication bias"
dat2 <- dat[dat$col_point != "red", ]

# they are the same
eq_8 <- lm(zi ~ prec, data = dat2)
eq_9 <- lm(yi ~ sei, weight = wi, data = dat2)
```

```{r}
summary(eq_8)
summary(eq_9)
```

As you can see above, Equation 8's intercept is identical the slope of Equation 9 whiile Equation 8's slope to Equation 9's intercept. Note that corresponding standard errors (SE) are also the same. 


## Appendix S3: Faile-safe *N* comparisions

We use the function `fsn()` in the R package, `metafor`, as in this web page  ([link](https://wviechtb.github.io/metafor/reference/fsn.html)).

```{r}
# an example data using risk ratio (not response ratio)
dat <- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
fsn(yi, vi, data=dat, type="Rosenthal")
# setting effect size targe as -0.1
fsn(yi, data=dat, type="Orwin", target = 0.1)
fsn(yi, vi, data=dat, type="Rosenberg")
```

## Appendix S4: Mulitlevel meta-regession method for publication bias

<!-- Alfredo + Yefeng - please give a quite overview of what we present her e please (as simple and short as possible) - please make sure to reference properly too - thanks -->

### S4.1: Correlation effect sizes (**Zr**)

```{r, results="hide"}
# importing dataset with correlation coefficients

dataset.r.original <- read.xlsx(here("data","ft044.xlsx"), colNames = TRUE, sheet = 1)

# transforming r to Zr for the analyses, and estimating VZr for the weighting
dataset.r.original$Sample.size <- as.numeric(dataset.r.original$Sample.size)

# subset dataset to those with 4 or more individuals (this removes two rows, one without sample size and one with sample size = 3)
dataset.r.original <- dataset.r.original[dataset.r.original$Sample.size>3 & !(is.na(dataset.r.original$Sample.size)),]

dataset.r.original$yi <- atanh(dataset.r.original$Correlation)
dataset.r.original$vi <- 1/(as.numeric(dataset.r.original$Sample.size)-3) #the effect based on 3 individuals won't be able to be analyzed because VZr is then infinite (see VZr function above to understand). Also, there is a missing sample size. 

# renaming some variables to make it easier
dataset.r.original$study <- dataset.r.original$Reference
dataset.r.original <- dataset.r.original[ , !names(dataset.r.original) %in% c("Reference")]
```

For our first worked example, we used the dataset provided by Garamszegi et al. (2012), who tested the hypothesis that behaviours are correlated across studies and species (behavioural syndrome), which they found support for. Here, we reanalysed the data from Garamszegi et al. (2012) by first conducting a phylogenetic multilevel intercept-only meta-analytic model and then testing for evidence of publication bias following the approaches outlined by Nakagawa et al. (main manuscript).

#### S4.1.1 - Phylogenetic relationships

Since multiple species (n = `r length(unique(dataset.r.original$Species))` species) are included in this dataset, we need to account for phylogenetic non-independence in our statistical models. For that, we build a phylogenetic tree for all these species (Figure S1) by retrieving their phylogenetic relationships from the Open Tree of Life (Hinchliff et al. 2015) using the R package `rotl` (Michonneau, Brown, and Winter 2016). We estimated branch lengths following Grafen (1989) as implemented in the function 'compute.brlen()' of the R package `ape` (Paradis and Schliep 2019). We then constructed a phylogenetic relatedness correlation matrix that will be fitted as part of the random effect structure of our models (see below).

```{r}
# First, we searched for the species names in the Open Tree Taxonomy (Rees and Cranston 2017) to confirm that all species names were correct, and that no synonyms or typos were present in the database.

# fixing a typo in the original list of species names
dataset.r.original$Species <- ifelse(dataset.r.original$Species=="Acrochephalus schoenobaenus",
                                     "Acrocephalus schoenobaenus",
                                     dataset.r.original$Species)

# updating a name that was not being well recognized by the Open Tree Taxonomy
dataset.r.original$Species <- ifelse(dataset.r.original$Species=="Eurotestudo boettgeri",
                                     "Testudo boettgeri",
                                     dataset.r.original$Species)

# fixing another typo in the original list of species names
dataset.r.original$Species <- ifelse(dataset.r.original$Species=="Taenopygia guttata",
                                     "Taeniopygia guttata",
                                     dataset.r.original$Species)

# # obtaining dataframe listing the Open Tree identifiers potentially matching our list of species (be aware that this will take a few minutes, and you can load the data below)
#taxa <- tnrs_match_names(names = unique(dataset.r.original$Species))

# saving the taxonomic data created on the 18th of February 2021 to speed the process in the future and make the code fully reproducible if taxonomic changes are implemented in the future
#save(taxa,file = "taxa_Open_Tree_of_Life_20210218.RData")

# # loading the taxonomic data (taxa) created on the 18th of February 2021
load(here("data", "taxa_Open_Tree_of_Life_20210218.RData"))

# # check approximate matches
# taxa[taxa$approximate_match==TRUE,]
# 
# # check synonyms matches
# taxa[taxa$is_synonym==TRUE,]
# 
# # check number of matches
# taxa[taxa$number_matches>1,]

# some further checks
ott_id_tocheck <- taxa[taxa$number_matches != 1,"ott_id"]

# for(i in 1:length(ott_id_tocheck)){
#   print(inspect(taxa, ott_id = ott_id_tocheck[i]))
# }

#all phylogenetic data seems in order now

# however, the ott_id for Parma unifasciata cannot be found when retrieving the phylogenetic relationships, so to trick this we are going to use the ott_id of another Parma species, Parma oligolepis (ott_id = 323186), which is this case it is fine because we only include two species belonging to the genus Parma, and therefore, the phylogenetic relationship will be the same for our purposes
# tnrs_match_names(names = 'Parma oligolepis')
taxa[taxa$unique_name=="Parma unifasciata","ott_id"] <- 323186

# retrieving phylogenetic relationships among taxa in the form of a trimmed sub-tree
tree <- tol_induced_subtree(ott_ids = taxa[["ott_id"]], label_format = "name")

# # we need to check for the existence of polytomies
# is.binary.tree(tree) # No polytomies, so we can proceed.

# to confirm that our tree covers all the species we wanted it to include, and make sure that the species names in our database match those in the tree, we use the following code

tree$tip.label <- gsub("_"," ", tree$tip.label)
# intersect(as.character(tree$tip.label), as.character(dataset.r.original$Species))
# setdiff(as.character(dataset.r.original$Species), as.character(tree$tip.label)) #listed in our database but not in the tree
# setdiff(as.character(tree$tip.label),as.character(dataset.r.original$Species)) # listed in the tree but not in our database

# All but Pan and Parma oligolepis are the same species, the "problem" is that synonyms have been used in the tree. We are going to leave all the names as in Open Tree of Life except Pan and Parma oligolepis, which we are going to substitute by Pan troglodytes and Parma unifasciata

# we start by fixing the following names in the tree
tree$tip.label[tree$tip.label=="Pan"]<-"Pan troglodytes"
tree$tip.label[tree$tip.label=="Parma oligolepis"]<-"Parma unifasciata"

# setdiff(as.character(dataset.r.original$Species), as.character(tree$tip.label)) #listed in our database but not in the tree
# setdiff(as.character(tree$tip.label),as.character(dataset.r.original$Species)) # listed in the tree but not in our database


# changing the names in our database to follow those in the tree. We are creating a new Species.updated variable so that it is clear that this list of Species is an updated version compared to the original one
dataset.r.original$Species.updated <- dataset.r.original$Species

dataset.r.original$Species.updated <- ifelse(dataset.r.original$Species.updated=="Carduelis chloris",
                                             "Chloris chloris",
                                             dataset.r.original$Species.updated)

dataset.r.original$Species.updated <- ifelse(dataset.r.original$Species.updated=="Dendroica pensylvaniaca",
                                             "Setophaga pensylvanica",
                                             dataset.r.original$Species.updated)

dataset.r.original$Species.updated <- ifelse(dataset.r.original$Species.updated=="Sylvia melanocephala",
                                             "Curruca melanocephala",
                                             dataset.r.original$Species.updated)

# setdiff(as.character(dataset.r.original$Species.updated), as.character(tree$tip.label)) #listed in our database but not in the tree
# setdiff(as.character(tree$tip.label),as.character(dataset.r.original$Species.updated)) # listed in the tree but not in our database

# all in order

# we can now save the tree
#save(tree, file = "tree_20211802.Rdata")

dataset.r <- dataset.r.original

```


```{r}
# load the saved tree (tree)
load(here("data","tree_20211802.Rdata")) 

# compute branch lengths of tree
phylo_branch <- compute.brlen(tree, method = "Grafen", power = 1)

# # check tree is ultrametric
# is.ultrametric(phylo_branch) # TRUE

# matrix to be included in the models
phylo_cor <- vcv(phylo_branch, cor = T)
```

##### Figure S4.1

Phylogenetic tree of all species included in this dataset. Notice that some of the names shown in the tree correspond to the most updated synonyms according to the Open Tree Taxonomy (Rees and Cranston 2017) of those originally avialable in the dataset provided by Garamszegi et al. (2012).

<br/><br/>
```{r, fig.height = 9, fig.width = 9.5, fig.align = "center"}
# we can then plot the tree
plot(tree, type = "fan", cex=0.65, label.offset =.05, no.margin = TRUE) #check: https://www.rdocumentation.org/packages/ape/versions/5.3/topics/plot.phylo
```

#### S4.1.2: Mulilevel-meta-analysis

Then, we used the data from Garamszegi et al. (2012) to provide a worked example on how to test of publication biases in datasets with several layers of non-independence and high heterogeneity (more in the main text), which is common for meta-analyses in ecology and evolution (e.g. Senior et al. 2016; Noble et al. 2017). Detailed results of the meta-analysis are shown in Table S4.1.

```{r}
# From the article:

# creating a unit-level random effect to model residual variance in metafor
dataset.r$obsID <- 1:nrow(dataset.r)

# running multilevel intercept-only meta-analytic model
meta.analysis.model.1 <- rma.mv(yi, vi,
                                mods=~1,
                                random=list(~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                method="REML",
                                test="t", # using t dist rather than z
                                data=dataset.r)

#summary(meta.analysis.model.1
# extracting the mean, 95% confidence intervals and 95% prediction intervals
#print(meta.analysis.model.1, digits=3)
metaanalytic.mean.model.1 <- predict(meta.analysis.model.1, digits=3)
#forest(meta.analysis.model.1)

# and absolute heterogeneity Q
#Q.model.1 <- c(meta.analysis.model.1$QM)

I2.model.1 <- i2_ml(meta.analysis.model.1, method = "wv")

# creating a table to show the heterogeneity estimates
# we exclude Q test as this is not so meaningful without DF and p (it is Chi-square values givne a df)
table.model.1 <- data.frame(n=length(unique(dataset.r.original$study)),
                            k=nrow(dataset.r.original),
                            mean=round(metaanalytic.mean.model.1[[1]],2),
                            CI=paste0("[",round(metaanalytic.mean.model.1[[3]],2),",",round(metaanalytic.mean.model.1[[4]],2),"]"),
                            PI=paste0("[",round(metaanalytic.mean.model.1[[5]],2),",",round(metaanalytic.mean.model.1[[6]],2),"]"),
                            I2_obsID=round(I2.model.1[["I2_obsID"]]*100,1),
                            I2_paperID=round(I2.model.1[["I2_study"]]*100,1),
                            I2_nonphylo=round(I2.model.1[["I2_Species"]]*100,1), 
                            I2_phylo=round(I2.model.1[["I2_Species.updated"]]*100,1),
                            I2_total=round(I2.model.1[["I2_total"]]*100,1)
                            )

rownames(table.model.1) <- NULL

# creating a fancy table using the R package 'gt'
table.model.1.gt <- table.model.1 %>% 
  gt() %>% 
  cols_label(n=md("**n**"),
             k=md("**k**"),
             mean=md("**Meta-analytic mean**"),
             CI=md("**95% CI**"),
             PI=md("**95% PI**"),
             I2_obsID=md("***I*<sup>2</sup><sub>residual</sub>\n(%)**"),
             I2_paperID=md("***I*<sup>2</sup><sub>study</sub>\n(%)**"),
             I2_nonphylo=md("***I*<sup>2</sup><sub>non-phylogeny</sub>\n(%)**"),
             I2_phylo=md("***I*<sup>2</sup><sub>phylogeny</sub>\n(%)**"),
             I2_total=md("***I*<sup>2</sup><sub>total</sub>    \n(%)**"),
             ) %>%
  cols_align(align = "center") %>%
  tab_source_note(source_note = md("n = number of studies; k = number of effects; CI = confidence interval; PI = prediction interval; *I*<sup>2</sup> = heterogeneity; *I*<sup>2</sup><sub>phylogeny</sub> is relates to phylogenetic heritability or *H*<sup>2</sup>.")) #%>%
 # tab_options(table.width=770)

table.model.1.gt

```

**Table S1.** Results of the phylogenetic multilevel intercept-only meta-analysis testing the relationship between behaviours across species. Estimates are presented as standardized effect sizes using Fisher’s transformation (i.e. *Zr*).

<br/><br/>

#### S4.1.3: Publication bias tests with mulitlevel meta-regression

##### S4.1.3.1: Meta-regression with SE (uni-moderator)

Something to be written here

<!-- in practice, we would not do 2 step for uni-variate models unless we assume homogeneity!, which we never do -->
<!-- Sorry - this is not where 2 step one comes in - yet - To test for small-study effects, we are going to follow the two-step approach that we suggested in the main text. First, we run a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the effect sizes' standard errors (SE or sei) as the only moderator (see Equation 21 from the main text). If the slope of this moderator is statistically significant, then we run an additional multilevel meta-regression including the effect sizes' sampling variances (SV or vi) as the only moderator (see Equation 22 from the main text), which allows us to obtain an overall effect (the intercept or B0) after accounting for small-study bias, and this overall is less downwardly  biased when modeling sampling variance than when modeling standard errors (more in the main text; REF - Doucouliagos 2012, 2014). -->

```{r}
# creating a varible for the standard error of each effect size (i.e. the square root of the sampling variance, see Figure 1 from the main manuscript)
dataset.r.original$sei <- sqrt(dataset.r$vi)

# Application of Equation 21 from the main manuscript
publication.bias.model.1.se <- rma.mv(yi, vi,
                                      mod = 1 + sei,
                                      random=list(~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                      R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                      method="REML",
                                      test="t", # using t dist rather than z
                                      data=dataset.r)

#print(publication.bias.model.1.se,digits=3)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.1.se <- estimates.CI(publication.bias.model.1.se)
```

What we need to write here is that - this gives some indication but it is best we look at this once we account for heterogneiety

Not sure the fig is needed yet but may be fine

<!--According to the meta-regression, there is evidence of small-study effects since the slope of the moderator (i.e. SE) is marginally statistically significant (**slope = `r round(estimates.publication.bias.model.1.se[2,2],2)`, 95% CI = [`r round(estimates.publication.bias.model.1.se[2,3],2)`,`r round(estimates.publication.bias.model.1.se[2,4],2)`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.1.se)[1]*100,1)`%; Figure S1), showing that effect sizes with larger SE are larger, and thus, showing evidence that some small effect sizes with large SE are seemingly missing. Since this meta-regression shows some evidence of small-study bias, we can the proceed to run the meta-regression suggested in Equation 22 from the main text, so that we can estimate an overall effect size for the meta-analysis after accouting for the existence of small-study bias.-->

```{r fig.height = 5.5, fig.width = 7.5, fig.align = "center"}
publication.bias.model.1.se.plot <- predict(publication.bias.model.1.se)

newdat <- data.frame(sei=dataset.r.original$sei,
                     fit=publication.bias.model.1.se.plot$pred,
                     upper=publication.bias.model.1.se.plot$ci.ub,
                     lower=publication.bias.model.1.se.plot$ci.lb,
                     stringsAsFactors=FALSE)

#newdat <- unique(newdat[order(newdat$sei),])
newdat <- unique(newdat[order(newdat$sei),])

xaxis <- dataset.r.original$sei
yaxis <- dataset.r.original$yi

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n",
     ylim=c(-2.25,2.25),
     xlim=c(0,1))


abline(a=0,b=0, lwd=1, lty=1)


axis(1,at=seq(0,1,0.1),
     cex.axis=0.8,tck=-0.02)

axis(2,
     at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "standard error (SE)", 
      ylab = "effect size (Zr)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=1)

lines(newdat$sei, newdat$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat$sei,rev(newdat$sei)),
        c(newdat$lower,rev(newdat$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S2.** Effect sizes with larger SE are larger, which provides evidence of small-study effects in this meta-analytic dataset. The solid line represents the model estimate and shading shows the 95% confidence intervals.


##### S4.1.3.2: Meta-regression with `year` (uni-moderator)

To test for time-lag bias, also called decline effects, we are again going to fit a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the year of publication as the only moderator (see Equation 23 from the main text). The estimated slope for year will tell us whether effect sizes have become smaller over time since the first effect size was published.

```{r}
# creating a variable with the year of publication
dataset.r$year <- as.numeric(str_extract(dataset.r$study, "(\\d)+"))

# Application of Equation 23 from the main manuscript
publication.bias.model.1.timelag <- rma.mv(yi, vi,
                                           mods=~1+year,
                                           random=list(~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                      R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                      method="REML",
                                      test="t", # using t dist rather than z
                                      data=dataset.r)

#summary(publication.bias.model.1.timelag)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.1.timelag <- estimates.CI(publication.bias.model.1.timelag)

```

the same as above - but this is unlikley to change even in the full model

Again not sure whether fig is needed but we can keep

<!--The time-lag bias test shows no evidence for a decline effect in this dataset (**year slope = `r round(estimates.publication.bias.model.1.timelag[2,2],2)`; 95% CI = [`r round(estimates.publication.bias.model.1.timelag[2,3],2)`,`r round(estimates.publication.bias.model.1.timelag[2,4],2)`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.1.timelag)[1]*100,1)`%; Figure S2).-->

```{r fig.height = 5.5, fig.width = 7.5, fig.align = "center"}
publication.bias.model.1.timelag.plot <- predict(publication.bias.model.1.timelag,newmods=seq(min(dataset.r.original$year),max(dataset.r.original$year),1))

newdat <- data.frame(year=seq(min(dataset.r.original$year),max(dataset.r.original$year),1),
                     fit=publication.bias.model.1.timelag.plot$pred,
                     upper=publication.bias.model.1.timelag.plot$ci.ub,
                     lower=publication.bias.model.1.timelag.plot$ci.lb,
                     stringsAsFactors=FALSE)


xaxis <- dataset.r$year
yaxis <- dataset.r$yi
cex.study <- (1/dataset.r$sei)/3

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n",
     ylim=c(-2.25,2.25),
     xlim=c(min(xaxis),max(xaxis)))


abline(a=0,b=0, lwd=1, lty=1)


axis(1,at=seq(min(xaxis),max(xaxis),5),
     cex.axis=0.8,tck=-0.02)

axis(2,
     at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "year of publication", 
      ylab = "effect size (Zr)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=cex.study)

lines(newdat$year, newdat$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat$year,rev(newdat$year)),
        c(newdat$lower,rev(newdat$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S3.** The overall published effect size has remained seemingly unchanged over time since the first effect size was published. The solid line represents the model estimate, shading shows the 95% confidence intervals and individual effect sizes are scaled by their precision (1/SE).

<br/><br/>

##### S4.1.3.3: All-in publication bias test (mulit-moderators)

When heterogeneity exists, it is best to combine Equations 21 and 23 with other moderators since those additional moderators will generally explain or be predicted to explain some of the heterogeneity among effect sizes. Therefore, in this case we will run a multilevel meta-regression including the effect sizes' standard errors, the year of publication and the following moderators originally included in Garamszegi et al. (2012): 'taxon class' and 'captivity status' (which accounted for 18.8% of heteregeneity, model not shown).

```{r}
# Application of Equation 24 from the main manuscript
#TODO a few things to notice - we need to make the intercept meaning full; using - 1 but this only works if we have one categorical varaible
# easiest to get the weighted mean of 

publication.bias.model.1.all.se <- rma.mv(yi, vi,
                                            mods=~sei+
                                            scale(year)+
                                            CaptivityC - 1, # take Class - that is already a part of phylo   
                                            random=list(~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                      R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                      method="REML",
                                      test="t", # using t dist rather than z
                                      data=dataset.r.original)

summary(publication.bias.model.1.all.se)

```

```{r}
# Application of Equation 24 from the main manuscript
#TODO a few things to notice - we need to make the intercept meaning full
# if marginal, this intercept is still the best estimat 

publication.bias.model.1.v <- rma.mv(yi, vi,
                                           mods=~vi
                                           scale(year)+
                                           CaptivityC - 1, # take Class - that is already a part of phylo   
                                           random=list(~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                      R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                      method="REML",
                                      test="t", # using t dist rather than z
                                      data=dataset.r.original)

summary(publication.bias.model.1.v)

# checking correltions among fixed effects
#model <- lmer(yi ~ vi + scale(year) + CaptivityC + (1|study), weights = 1/vi, data = dataset.r.original)
#summary(model)

# averaging all the 
# res.dat <- data.frame(yi = publication.bias.model.1.v$b[-c(1,2)], vi =  publication.bias.model.1.v$se[-c(1,2)]^2, cluster = rep("Captitive", 5))
# dat <- escalc(yi=yi, vi=vi, data=res.dat)
# test <- aggregate(dat, cluster = cluster, rho = 0)
# compare with this
# TODO - I need to think about what's the best

publication.bias.model.1.v2 <- rma.mv(yi, vi,
                                           mods=~vi + 
                                           scale(year),
                                           random=list(~1 | CaptivityC,
                                             ~ 1 | Species.updated, # phylo effect
                                            ~ 1 | Species, # non-phylo effect
                                            ~ 1 | study, 
                                            ~ 1 | obsID), 
                                      R = list(Species.updated = phylo_cor), #phylogenetic relatedness
                                      method="REML",
                                      test="t", # using t dist rather than z
                                      data=dataset.r.original)

summary(publication.bias.model.1.v2)

```

**Need to add a little text and probably also a table showing the results of this meta-regresssion, which I did not do because it would be very specific for the dataset we will finally used. Nonetheless, find below the *R<sup>2</sup><sub>marginal</sub>* estimate:**

*R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.1.timelag.all.in)[1]*100,1)`%

### S4.2: Mean differences between two groups (lnRR and SMD)

```{r}
# importing dataset with mean differences
dataset.original.mean.diff<-read_excel(here("data", "ft027.xlsx"), col_names = TRUE)

#names(dataset.original.mean.diff)
#str(dataset.original.mean.diff)


# lnRR
dataset.rr  <- escalc(measure = "ROM",
                                m1i = T_mean,
                                m2i = C_mean,
                                sd1i = T_sd,
                                sd2i = C_sd,
                                n1i = T_N,
                                n2i = C_N,
                        data = dataset.original.mean.diff,
                        append = T)

dataset.smd <- escalc(measure = "SMD",
                                m1i = T_mean,
                                m2i = C_mean,
                                sd1i = T_sd,
                                sd2i = C_sd,
                                n1i = T_N,
                                n2i = C_N,
                     data = dataset.original.mean.diff,
                        append = T)
  

# metrics <- data.frame(RR = lnRR$yi, VRR = lnRR$vi, SMD = SMD$yi, VSMD = SMD$vi)
# dataset.mean.diff <- cbind(dataset.original.mean.diff, metrics) # bind_cols()

# clean NA
  dataset.rr <- dataset.rr[!is.na(dataset.rr$yi) & dataset.rr$vi != 0, ]
  
  dataset.smd <- dataset.smd[!is.na(dataset.smd$yi) & dataset.smd$vi != 0, ]
  
```


#### S4.2.1: Meta-analytic model

<!--XXX et al. (XXXX) tested the hypothesis that X and Y were correlated across studies and species. Indeed, the authors found support for their hypothesis, showing that X and Y were correlated across studies and species. Here, we reanalysed the data from XXX et al. (XXXX) by conducting a multilevel intercept-only meta-analytic model. Then, we used the data from XXX et al. (XXXX) to provide a worked example on how to test of publication biases in datasets with several layers of non-independence and high heterogeneity (more in the main text), which is common for meta-analyses in ecology and evolution (e.g. Senior et al. 2016; Noble et al. 2017). Detailed results of the meta-analysis are shown in Table S1.-->

```{r}
# creating a unit-level random effect to model residual variance in metafor
dataset.rr$obsID <- 1:nrow(dataset.rr)
dataset.smd$obsID <- 1:nrow(dataset.smd)

# running multilevel intercept-only meta-analytic model
meta.analysis.model.RR <- rma.mv(yi, vi,
                                mods = ~ 1,
                                random = list(~ 1 | paperID, 
                                              ~ 1 | obsID),
                                method = "REML",
                                test = "t",
                                data = dataset.rr)


meta.analysis.model.SMD <- rma.mv(yi, vi,
                                mods = ~ 1,
                                random = list(~ 1 | paperID, 
                                              ~ 1 | obsID),
                                method = "REML",
                                test = "t",
                                data = dataset.smd)

# extracting the mean, 95% confidence intervals and 95% prediction intervals
#print(meta.analysis.model.1, digits=3)
metaanalytic.mean.model.RR <- predict(meta.analysis.model.RR, digits=3)
metaanalytic.mean.model.SMD <- predict(meta.analysis.model.SMD, digits=3)
#forest(meta.analysis.model.1)

# estimating relative heterogeneity I2
I2.model.RR <- i2_ml(meta.analysis.model.RR)*100
I2.model.SMD <- i2_ml(meta.analysis.model.SMD)*100

# and absolute heterogeneity Q
#Q.model.RR <- c(meta.analysis.model.RR$QE)
#Q.model.SMD <- c(meta.analysis.model.SMD$QE)

# creating a table to show the heterogeneity estimates
table.model.RR <- data.frame(n=length(unique(dataset.mean.diff$paperID)),
                            k=nrow(dataset.mean.diff),
                            mean=round(metaanalytic.mean.model.RR[[1]],2),
                            CI=paste0("[",round(metaanalytic.mean.model.RR[[3]],2),",",round(metaanalytic.mean.model.RR[[4]],2),"]"),
                            PI=paste0("[",round(metaanalytic.mean.model.RR[[5]],2),",",round(metaanalytic.mean.model.RR[[6]],2),"]"),
                            I2_obsID=round(I2.model.RR[[3]],1),
                            I2_paperID=round(I2.model.RR[[2]],1),
                            I2_total=round(I2.model.RR[[2]],1))

rownames(table.model.RR) <- NULL

#write.csv(table.model.RR, "./table/table.model.RR.csv", row.names = FALSE)

table.model.SMD <- data.frame(n=length(unique(dataset.mean.diff$paperID)),
                            k=nrow(dataset.mean.diff),
                            mean=round(metaanalytic.mean.model.SMD[[1]],2),
                            CI=paste0("[",round(metaanalytic.mean.model.SMD[[3]],2),",",round(metaanalytic.mean.model.SMD[[4]],2),"]"),
                            PI=paste0("[",round(metaanalytic.mean.model.SMD[[5]],2),",",round(metaanalytic.mean.model.SMD[[6]],2),"]"),
                            I2_obsID=round(I2.model.SMD[[3]],1),
                            I2_paperID=round(I2.model.SMD[[2]],1),
                            I2_total=round(I2.model.SMD[[1]],1))

rownames(table.model.SMD) <- NULL

#write.csv(table.model.SMD, "./table/table.model.SMD.csv", row.names = FALSE)

# creating a fancy table using the R package 'gt'
table.model.RR.gt <- table.model.RR %>% 
  gt() %>% 
  cols_label(n=md("**n**"),
             k=md("**k**"),
             mean=md("**Meta-analytic mean**"),
             CI=md("**95% CI**"),
             PI=md("**95% PI**"),
             I2_obsID=md("***I*<sup>2</sup><sub>residual</sub> (%)**"),
             I2_paperID=md("***I*<sup>2</sup><sub>study</sub> (%)**"),
             I2_total=md("***I*<sup>2</sup><sub>total</sub> (%)**")) %>%
  cols_align(align = "center") %>%
  tab_source_note(source_note = md("n = number of studies; k = number of effects; CI = confidence interval; PI = prediction interval; *I*<sup>2</sup> = heterogeneity")) #%>%
  #tab_options(table.width=775)

table.model.RR.gt


table.model.SMD.gt <- table.model.SMD %>% 
  gt() %>% 
cols_label(n=md("**n**"),
             k=md("**k**"),
             mean=md("**Meta-analytic mean**"),
             CI=md("**95% CI**"),
             PI=md("**95% PI**"),
             I2_obsID=md("***I*<sup>2</sup><sub>residual</sub> (%)**"),
             I2_paperID=md("***I*<sup>2</sup><sub>study</sub> (%)**"),
             I2_total=md("***I*<sup>2</sup><sub>total</sub> (%)**")) %>%
  cols_align(align = "center") %>%
  tab_source_note(source_note = md("n = number of studies; k = number of effects; CI = confidence interval; PI = prediction interval; *I*<sup>2</sup> = heterogeneity"))  #%>%
  #tab_options(table.width=775)

table.model.SMD.gt

```

**Table S1.** Results of the multilevel intercept-only meta-analysis testing the relationship between XXX and YYY across species. Estimates are presented as standardized effect sizes using log transformed ratio of means (i.e., lnRR).standardized mean difference (i.e., Hedges' g)

<br/><br/>

**We could also provide an orchard plot of the results. The reason why I did not do it is because I need to update R to do that and have not found the time. If you'd like to provide an orchard plot, maybe the fastest would be for Yefeng to do that**

#### S4.2.2: Publication bias tests with mulitlevel meta-regression

Something here?

##### S4.2.2.1: Meta-regression with effective sampling size (uni-moderator)

Here try not repeat what you already said - you can pretty much say as above and what's differet

<!--To test for small-study effects, we are going to follow the two-step approach that we suggested in the main text. First, we run a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the effect sizes' standard errors (SE or sei) as the only moderator (see Equation 21 from the main text). If the slope of this moderator is statistically significant, then we run an additional multilevel meta-regression including the effect sizes' sampling variances (SV or vi) as the only moderator (see Equation 22 from the main text), which allows us to obtain an overall effect (the intercept or B0) after accounting for small-study bias, and this overall is less downwardly  biased when modeling sampling variance than when modeling standard errors (more in the main text; Doucouliagos 2012, 2014).-->

```{r}
# calculating "effective sample size" to account for unbalanced sampling, for SMD and lnRR (esz, hereafter, see Equation 25 from the main manuscript)

dataset.rr$inv_n_tilda <-  (dataset.rr$C_N + dataset.mean.diff$T_N)/(dataset.rr$C_N*dataset.rr$T_N)

dataset.smd$inv_n_tilda <-  (dataset.smd$C_N + dataset.smd$T_N)/(dataset.smd$C_N*dataset.smd$T_N)

# Application of Equation 27 from the main manuscript
publication.bias.model.RR.srin <- rma.mv(yi, vi,
                                      mods= ~ 1 + sqrt(inv_n_tilda),
                                      random=list(~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.rr)
#publication.bias.model.RR.srin

publication.bias.model.SMD.srin <- rma.mv(yi, vi,
                                      mods= ~ 1 + sqrt(inv_n_tilda),
                                      random=list(~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.smd)

#publication.bias.model.SMD.srin

#print(publication.bias.model.1.se,digits=3)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.RR.srin <- estimates.CI(publication.bias.model.RR.srin)

estimates.publication.bias.model.SMD.srin <- estimates.CI(publication.bias.model.SMD.srin)

```
This is an interesting example where sig is different between lnRR and SMD but we should not be so exicted yet - we need to control for heterogeniety first

probably change  below

<!--According to the meta-regression, there is evidence of small-study effects since the slope of the moderator (i.e. SE) is statistically significant (**slope = `r estimates.publication.bias.model.RR.se[2,2]`, 95% CI = [`r estimates.publication.bias.model.1.se[2,3]`,`r estimates.publication.bias.model.RR.se[2,4]`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.RR.se)[1]*100,1)`%; Figure S1), showing that effect sizes with larger SE are larger, and thus, showing evidence that some small effect sizes with large SE are seemingly missing. Since this meta-regression shows some evidence of small-study bias, we  proceed to run the meta-regression suggested in Equation 28 from the main text, so that we can estimate an overall effect size for the meta-analysis after accouting for the existence of small-study bias.-->

**For the time being, I have assumed that the slope for SE is statistically significant, which is not in this dataset, but should be for the final chosen example. That is, even the wording it's based on that, i.e. the results for this specific example do not agree with the description of the results. APOLOGIES if it is confussing**

```{r}
publication.bias.model.RR.se.plot <- predict(publication.bias.model.RR.se)

newdat.RR <- data.frame(sei=dataset.mean.diff$RR.esz.sei,
                     fit=publication.bias.model.RR.se.plot$pred,
                     upper=publication.bias.model.RR.se.plot$ci.ub,
                     lower=publication.bias.model.RR.se.plot$ci.lb,
                     stringsAsFactors=FALSE)

#newdat <- unique(newdat[order(newdat$sei),])
newdat.RR <- unique(newdat.RR[order(newdat.RR$sei),])

xaxis <- dataset.mean.diff$RR.esz.sei
yaxis <- dataset.mean.diff$RR

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n"
     #ylim=c(-1,2),
     #xlim=c(0,1.1)
     )


abline(a=0,b=0, lwd=1, lty=1)


axis(1,at=seq(0,1,0.1),
     cex.axis=0.8,tck=-0.02)

axis(2,
     #at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "Effective sample size based standard error (SE)", 
      ylab = "Effect size (lnRR)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=1)

lines(newdat.RR$sei, newdat.RR$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat.RR$sei,rev(newdat.RR$sei)),
        c(newdat.RR$lower,rev(newdat.RR$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S1.** Effect sizes with larger SE are larger, which provides evidence of small-study effects in this meta-analytic dataset. The solid line represents the model estimate and shading shows the 95% confidence intervals.



##### S4.2.2.2: Time-lag bias test (uni-moderator)

To test for time-lag bias, also called decline effects, we are again going to fit a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the year of publication as the only moderator (see Equation 23 from the main text). The estimated slope for year will tell us whether effect sizes have become smaller over time since the first effect size was published.

```{r}
# Application of Equation 23 from the main manuscript
publication.bias.model.RR.timelag <- rma.mv(yi, vi,
                                      mods= ~ 1 + year,
                                      random=list(~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.rr)

publication.bias.model.SDM.timelag <- rma.mv(yi, vi,
                                      mods= ~ 1 + year,
                                      random=list(~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.smd)

#summary(publication.bias.model.1.timelag)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.RR.timelag <- estimates.CI(publication.bias.model.RR.timelag)
estimates.publication.bias.model.SMD.timelag <- estimates.CI(publication.bias.model.SDM.timelag)
```

<!--The time-lag bias test indeed shows evidence for a decline effects since the slope of year is negative, meaning that effect sizes have trended towards zero over time (**year slope = `r estimates.publication.bias.model.RR.timelag[2,2]`; 95% CI = [`r estimates.publication.bias.model.RR.timelag[2,3]`,`r estimates.publication.bias.model.RR.timelag[2,4]`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.RR.timelag)[1]*100,1)`%; Figure S2).

```{r}
publication.bias.model.RR.timelag.plot <- predict(publication.bias.model.RR.timelag,newmods=seq(min(dataset.mean.diff$Year),max(dataset.mean.diff$Year),1))

newdat.RR <- data.frame(year=seq(min(dataset.mean.diff$Year),max(dataset.mean.diff$Year),1),
                     fit=publication.bias.model.RR.timelag.plot$pred,
                     upper=publication.bias.model.RR.timelag.plot$ci.ub,
                     lower=publication.bias.model.RR.timelag.plot$ci.lb,
                     stringsAsFactors=FALSE)


xaxis <- dataset.mean.diff$Year
yaxis <- dataset.mean.diff$RR
cex.study <- 1/dataset.mean.diff$RR.esz.sei

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n",
     ylim=c(-2.25,2.25),
     xlim=c(min(xaxis),max(xaxis))
     )


abline(a=0,b=0, lwd=1, lty=1)


axis(1, at=seq(min(xaxis),max(xaxis),5),
     cex.axis=0.8,tck=-0.02)

axis(2,
     #at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "Year of publication", 
      ylab = "Effect size (lnRR)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=cex.study)

lines(newdat.RR$year, newdat.RR$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat.RR$year,rev(newdat.RR$year)),
        c(newdat.RR$lower,rev(newdat.RR$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S2.** The overall published effect size has decreased over time since first described. The solid line represents the model estimate, shading shows the 95% confidence intervals and individual effect sizes are scaled by their precision (1/SE).-->

<br/><br/>


##### S4.2.2.3:  All-in publication bias test (mulit-moderators)

Try not to repeat what you do not need to 

<!--When heterogeneity exists, it is best to combine Equations 27 and 23 with other moderators since those additional moderators will generally explain or be predicted to explain some of the heterogeneity among effect sizes. Therefore, in this case we will run a multilevel meta-regression including the effective sample size based standard errors, the year of publication and the following moderators originally included in XXX et al. (XXXX): 'season', 'group composition' and 'type of interactions'.-->

```{r}
# preparing the moderators that need to be included in a meta-regression that also contains a  moderator with the standard errors of the effect sizes and the year of publication



# Application of Equation 29 from the main manuscript
publication.bias.model.RR.timelag.srin. <- rma.mv(yi, vi,
                                      mods= ~ 1 + sqrt(inv_n_tilda) + 
                                        scale(year) + 
                                        Experimental.or.Observational.,
                                      random=list(
                                        ~1 |Disturbance_category,
                                        ~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.rr,
                                      control=list(optimizer="optim", optmethod="Nelder-Mead"))


# none of these seem to be sig factors (explaining sig heterogeneity)
test <- rma.mv(yi, vi,
              mods= ~ 1 + Disturbance_category + 
                    Biome + 
                    Species.category..invader.,
              random=list(~ 1 | obsID, 
                         ~ 1 | paperID),
              method="REML",
              test = "t",
              data=dataset.rr,
              control=list(optimizer="optim", optmethod="Nelder-Mead")
               )

###
publication.bias.model.RR.timelag.all.eN <- rma.mv(yi, vi,
                                      mods= ~ 1 + inv_n_tilda + 
                                        scale(year) + 
                                        Experimental.or.Observational.,
                                      random=list(
                                        ~1 |Disturbance_category,
                                        ~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.rr,
                                      control=list(optimizer="optim", optmethod="Nelder-Mead"))

publication.bias.model.RR.timelag.all.eN1 <- rma.mv(yi, vi,
                                      mods= ~ 1 + inv_n_tilda + 
                                        scale(year),
                                      random=list(
                                        ~1 |Disturbance_category,
                                        ~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.rr,
                                      control=list(optimizer="optim", optmethod="Nelder-Mead"))
#####

# Application of Equation 29 from the main manuscript
publication.bias.model.SMD.timelag.al.srin <- rma.mv(yi, vi,
                                      mods= ~ 1 + sqrt(inv_n_tilda) + 
                                        scale(year) + 
                                        Experimental.or.Observational.,
                                      random=list(
                                        ~1 |Disturbance_category,
                                        ~ 1 | obsID, 
                                                  ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.smd,
                                      control=list(optimizer="optim", optmethod="Nelder-Mead"))

publication.bias.model.SMD.timelag.al.srin1 <- rma.mv(yi, vi,
                                      mods= ~ 1 + sqrt(inv_n_tilda) + 
                                        scale(year),
                                      random=list(
                                        ~1 |Disturbance_category,
                                        ~ 1 | obsID, 
                                        ~ 1 | paperID),
                                      method="REML",
                                      test = "t",
                                      data=dataset.smd,
                                      control=list(optimizer="optim", optmethod="Nelder-Mead"))


```

**Need to add a little text and probably also a table showing the results of this meta-regresssion, which I did not do because it would be very specific for the dataset we will finally used. Nonetheless, find below the *R<sup>2</sup><sub>marginal</sub>* estimate, which for this sparrow example is pretty high and much coming from the publication bias tests**

*R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.1.timelag.all.in)[1]*100,1)`%


## Appendix S5: Avearging and sampling approches for publication bias

Some intro - an overview 

1. averaging vs sampling?
2. what is trim and fill methods?
3. what is 3 PSM?

### S5.1: Correlation effect sizes (**Zr**)

#### S5.1.1: Averaging 

use aggregate

```{r}

agg <- aggregate(dataset.r, cluster=study, struct="ID")
agg
dataset.r

```


##### S5.1.1.2: Trim and fill test 

This may not work but do this anyways

Easy

##### S5.1.1.3: Selection model (3 PSM)

Easy

##### S5.1.1.4: Cumlative meta-analysis 

I guess we do not expect a decline effect - but we do this anyways

Easy.....

#### S5.1.2: Sampling 

##### S5.1.2.2: Trim and fill test 

##### S5.1.2.3: Selection model (3 PSM)

##### S5.1.2.4: Cumlative meta-analysis 

### S5.2: Mean differences between two groups (lnRR and SMD)

#### S5.2.1: Averaging 

##### S5.2.1.2: Trim and fill test 

##### S5.2.1.3: Selection model (3 PSM)

##### S5.2.1.4: Cumlative meta-analysis 

#### S5.2.2: Sampling 

##### S5.2.2.2: Trim and fill test 

##### S5.2.2.3: Selection model (3 PSM)

##### S5.2.2.4: Cumlative meta-analysis 

## Appendix extra: Figure's R code

Shinichi to do




## R Session Information

```{r}
sessionInfo() %>% pander()
```

## References

To be added.