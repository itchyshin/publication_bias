---
title: Methods for testing publication bias in ecological and evolutionary meta-analyses
author: Shinichi Nakagawa, Malgorzata Lagisz, Michael D. Jennions, Julia Koricheva, Daniel W.A. Noble, Timothy H. Parker, Alfredo Sanchez-Tojar, Yefeng Yang, Rose O'Dea
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    code_download: yes
    code_folding: hide
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
subtitle: Supplementary Information Appendix S2 - Publication bias tests for ecology & evolution
---

```{r,echo=FALSE, cache=FALSE}
## numbers >= 10^5 will be denoted in scientific notation, and rounded to 2 digits
options(digits=3)
```

```{r setup, echo = FALSE, eval = TRUE}
###################################################################
# Worked example publication bias methodology
#
# Code written by:
#
#         Alfredo Sanchez-Tojar (alfredo.tojar@gmail.com); Departament of Evolutionary Biology, Bielefeld University, Germany
#
#         Yefeng Yang (yangyefeng1/@UNSW.edu.cn);
#
# Code validated by:
#
#         ...
#
# Script first created on November 20th, 2020

###################################################################
# Description of script and Instructions
###################################################################

# This script is to generate a worked example to show how to test for
# publication bias when several layers of non-independence exist in
# the data, which is common for meta-analytic datasets in ecology 
# and evolution. We provide two worked examples, one for correlation
# effect sizes (r) and one for mean comparisons (SMD and lnRR).

rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE)
pacman::p_load(dplyr,metafor,gt,pander,readxl,readr,tidyverse)


# necessary custom functions

# Function to obtain I^2 (written by Shinichi Nakagawa)
I2 <- function(model, method = c("Wolfgang", "Shinichi")){
  warning("Make sure you have the observation (effec size) level random effect\n")
  ## evaluate choices
  method <- match.arg(method)
  
  # Wolfgang's method
  if(method == "Wolfgang"){
    W <- solve(model$V) 
    X <- model.matrix(model)
    P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
    I2_total <- sum(model$sigma2) / (sum(model$sigma2) + (model$k - model$p) / sum(diag(P)))
    I2_each  <- model$sigma2 / (sum(model$sigma2) + (model$k - model$p) / sum(diag(P)))
    names(I2_each) = paste0("I2_", model$s.names)
    
    # putting all together
    I2s <- c(I2_total = I2_total, I2_each)
    
    # or Shinichi Nakagawa
  } else {
    # sigma2_v = typical sampling error variance
    sigma2_v <- sum(1/model$vi) * (model$k-1) / (sum(1/model$vi)^2 - sum((1/model$vi)^2)) 
    I2_total <- sum(model$sigma2) / (sum(model$sigma2) + sigma2_v) #s^2_t = total variance
    I2_each  <- model$sigma2 / (sum(model$sigma2) + sigma2_v)
    names(I2_each) = paste0("I2_", model$s.names)
    
    # putting all together
    I2s <- c(I2_total = I2_total, I2_each)
  }
  return(I2s)
}

# Function to obtain marginal R^2 (written by Shinichi Nakagawa)
R2 <- function(model){
  warning("Make sure you have the observation (effec size) level random effect as the last in the formula\n")
  
  # fixed effect variance
  fix <- var(as.numeric(as.vector(model$b) %*% t(as.matrix(model$X))))
  
  # marginal
  R2m <- fix / (fix + sum(model$sigma2))
  R2
  
  # conditional
  R2c <- (fix + sum(model$sigma2) - model$sigma2[length(model$sigma2)]) / 
    (fix + sum(model$sigma2))
  
  R2s <- c(R2_marginal = R2m, R2_coditional = R2c)
  
  return(R2s)
}

# extracting mean and CI from each metafor model
estimates.CI <- function(model){
  db.mf <- data.frame(model$b,row.names = 1:nrow(model$b))
  db.mf <- cbind(db.mf,model$ci.lb,model$ci.ub,row.names(model$b))
  names(db.mf) <- c("mean","lower","upper","estimate")
  return(db.mf[,c("estimate","mean","lower","upper")])
}
```

## 1 - Mean difference effect sizes

```{r, results="hide"}
# importing dataset with mean differences
dataset.original.mean.diff<-read_excel("./ft027.xlsx", col_names = TRUE)

names(dataset.original.mean.diff)
str(dataset.original.mean.diff)


# lnRR
  lnRR <- with(dataset.original.mean.diff, escalc(measure = "ROM",
                                m1i = T_mean,
                                m2i = C_mean,
                                sd1i = T_sd,
                                sd2i = C_sd,
                                n1i = T_N,
                                n2i = C_N))

  SMD <- with(dataset.original.mean.diff, escalc(measure = "SMD",
                                m1i = T_mean,
                                m2i = C_mean,
                                sd1i = T_sd,
                                sd2i = C_sd,
                                n1i = T_N,
                                n2i = C_N))
  

metrics <- data.frame(RR = lnRR$yi, VRR = lnRR$vi, SMD = SMD$yi, VSMD = SMD$vi)
dataset.mean.diff <- cbind(dataset.original.mean.diff, metrics) # bind_cols()





# clean NA
  dataset.mean.diff <- dataset.mean.diff[!is.na(dataset.mean.diff$RR),]
  
  dataset.mean.diff <- dataset.mean.diff[!is.na(dataset.mean.diff$SMD),]
  



```

### 1.1 - Meta-analytic model

XXX et al. (XXXX) tested the hypothesis that X and Y were correlated across studies and species. Indeed, the authors found support for their hypothesis, showing that X and Y were correlated across studies and species. Here, we reanalysed the data from XXX et al. (XXXX) by conducting a multilevel intercept-only meta-analytic model. Then, we used the data from XXX et al. (XXXX) to provide a worked example on how to test of publication biases in datasets with several layers of non-independence and high heterogeneity (more in the main text), which is common for meta-analyses in ecology and evolution (e.g. Senior et al. 2016; Noble et al. 2017). Detailed results of the meta-analysis are shown in Table S1.

```{r}
# creating a unit-level random effect to model residual variance in metafor
dataset.mean.diff$obsID <- 1:nrow(dataset.mean.diff)


# running multilevel intercept-only meta-analytic model
meta.analysis.model.RR <- rma.mv(RR, VRR,
                                mods = ~ 1,
                                random = list(~ 1 | paperID, ~ 1 | obsID),
                                method = "REML",data = dataset.mean.diff)


meta.analysis.model.SMD <- rma.mv(SMD, VSMD,
                                mods = ~ 1,
                                random = list(~ 1 | paperID, ~ 1 | obsID),
                                method = "REML",data = dataset.mean.diff)

# extracting the mean, 95% confidence intervals and 95% prediction intervals
#print(meta.analysis.model.1, digits=3)
metaanalytic.mean.model.RR <- predict(meta.analysis.model.RR, digits=3)
metaanalytic.mean.model.SMD <- predict(meta.analysis.model.SMD, digits=3)
#forest(meta.analysis.model.1)

# estimating relative heterogeneity I2
I2.model.RR <- I2(meta.analysis.model.RR,method = c("Shinichi"))*100
I2.model.SMD <- I2(meta.analysis.model.SMD,method = c("Shinichi"))*100

# and absolute heterogeneity Q
Q.model.RR <- c(meta.analysis.model.RR$QE)
Q.model.SMD <- c(meta.analysis.model.SMD$QE)

# creating a table to show the heterogeneity estimates
table.model.RR <- data.frame(n=length(unique(dataset.mean.diff$paperID)),
                            k=nrow(dataset.mean.diff),
                            mean=round(metaanalytic.mean.model.RR[[1]],2),
                            CI=paste0("[",round(metaanalytic.mean.model.RR[[3]],2),",",round(metaanalytic.mean.model.RR[[4]],2),"]"),
                            PI=paste0("[",round(metaanalytic.mean.model.RR[[5]],2),",",round(metaanalytic.mean.model.RR[[6]],2),"]"),
                            I2_obsID=round(I2.model.RR[2],1),
                            I2_paperID=round(I2.model.RR[3],1),
                            #I2_popID2=round(I2.model.RR[4],1),
                            I2_total=round(I2.model.RR[1],1),
                            Q=round(Q.model.RR,1))

rownames(table.model.RR) <- NULL

write.csv(table.model.RR, "./table/table.model.RR.csv", row.names = FALSE)

table.model.SMD <- data.frame(n=length(unique(dataset.mean.diff$paperID)),
                            k=nrow(dataset.mean.diff),
                            mean=round(metaanalytic.mean.model.SMD[[1]],2),
                            CI=paste0("[",round(metaanalytic.mean.model.SMD[[3]],2),",",round(metaanalytic.mean.model.SMD[[4]],2),"]"),
                            PI=paste0("[",round(metaanalytic.mean.model.SMD[[5]],2),",",round(metaanalytic.mean.model.SMD[[6]],2),"]"),
                            I2_obsID=round(I2.model.SMD[2],1),
                            I2_paperID=round(I2.model.SMD[3],1),
                            #I2_popID2=round(I2.model.SMD[4],1),
                            I2_total=round(I2.model.SMD[1],1),
                            Q=round(Q.model.SMD,1))

rownames(table.model.SMD) <- NULL

write.csv(table.model.SMD, "./table/table.model.SMD.csv", row.names = FALSE)

# creating a fancy table using the R package 'gt'
table.model.RR.gt <- table.model.RR %>% 
  gt() %>% 
  cols_label(n=md("**n**"),
             k=md("**k**"),
             mean=md("**Meta-analytic mean**"),
             CI=md("**95% CI**"),
             PI=md("**95% PI**"),
             I2_obsID=md("***I*<sup>2</sup><sub>residual</sub> (%)**"),
             I2_paperID=md("***I*<sup>2</sup><sub>study</sub> (%)**"),
             #I2_popID2=md("***I*<sup>2</sup><sub>population</sub> (%)**"),
             #Phylogeny=md("***I*<sup>2</sup><sub>Phylo</sub> (%)**"),
             I2_total=md("***I*<sup>2</sup><sub>total</sub> (%)**"),
             Q=md("***Q*<sub>test</sub>**")) %>%
  cols_align(align = "center") %>%
  tab_source_note(source_note = md("n = number of studies; k = number of effects; CI = confidence interval; PI = prediction interval; *I*<sup>2</sup> = heterogeneity; *Q*<sub>test</sub> = Cochrane's *Q* test.")) %>%
  tab_options(table.width=775)

table.model.RR.gt



table.model.SMD.gt <- table.model.SMD %>% 
  gt() %>% 
  cols_label(n=md("**n**"),
             k=md("**k**"),
             mean=md("**Meta-analytic mean**"),
             CI=md("**95% CI**"),
             PI=md("**95% PI**"),
             I2_obsID=md("***I*<sup>2</sup><sub>residual</sub> (%)**"),
             I2_paperID=md("***I*<sup>2</sup><sub>study</sub> (%)**"),
             #I2_popID2=md("***I*<sup>2</sup><sub>population</sub> (%)**"),
             #Phylogeny=md("***I*<sup>2</sup><sub>Phylo</sub> (%)**"),
             I2_total=md("***I*<sup>2</sup><sub>total</sub> (%)**"),
             Q=md("***Q*<sub>test</sub>**")) %>%
  cols_align(align = "center") %>%
  tab_source_note(source_note = md("n = number of studies; k = number of effects; CI = confidence interval; PI = prediction interval; *I*<sup>2</sup> = heterogeneity; *Q*<sub>test</sub> = Cochrane's *Q* test.")) %>%
  tab_options(table.width=775)

table.model.SMD.gt

```

**Table S1.** Results of the multilevel intercept-only meta-analysis testing the relationship between XXX and YYY across species. Estimates are presented as standardized effect sizes using log transformed ratio of means (i.e., lnRR).standardized mean difference (i.e., Hedges' g)

<br/><br/>

**We could also provide an orchard plot of the results. The reason why I did not do it is because I need to update R to do that and have not found the time. If you'd like to provide an orchard plot, maybe the fastest would be for Yefeng to do that**


### 1.2 - Publication bias

#### **1.2.1 - The two-step approach**

To test for small-study effects, we are going to follow the two-step approach that we suggested in the main text. First, we run a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the effect sizes' standard errors (SE or sei) as the only moderator (see Equation 21 from the main text). If the slope of this moderator is statistically significant, then we run an additional multilevel meta-regression including the effect sizes' sampling variances (SV or vi) as the only moderator (see Equation 22 from the main text), which allows us to obtain an overall effect (the intercept or B0) after accounting for small-study bias, and this overall is less downwardly  biased when modeling sampling variance than when modeling standard errors (more in the main text; Doucouliagos 2012, 2014).

```{r}
# calculating "effective sample size" to account for unbalanced sampling, for SMD and lnRR (esz, hereafter, see Equation 25 from the main manuscript)
dataset.mean.diff$RR.esz <- (4*dataset.mean.diff$C_N*dataset.mean.diff$T_N) / (dataset.mean.diff$C_N + dataset.mean.diff$T_N)

dataset.mean.diff$SMD.esz <- (4*dataset.mean.diff$C_N*dataset.mean.diff$T_N) / (dataset.mean.diff$C_N + dataset.mean.diff$T_N)

# creating "effective sample size" based "sampling variance" (see Equation 26)
dataset.mean.diff$RR.esz.sv <- 4/dataset.mean.diff$RR.esz

dataset.mean.diff$SMD.esz.sv <- 4/dataset.mean.diff$SMD.esz

# creating corresponding "standard error" (i.e. the square root of the sampling variance)
dataset.mean.diff$RR.esz.sei <- sqrt(dataset.mean.diff$RR.esz.sv)

dataset.mean.diff$SMD.esz.sei <- sqrt(dataset.mean.diff$SMD.esz.sv)

# Application of Equation 27 from the main manuscript
publication.bias.model.RR.se <- rma.mv(RR, VRR,
                                      mods= ~ 1 + RR.esz.sei,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)


publication.bias.model.SMD.se <- rma.mv(SMD, VSMD,
                                      mods= ~ 1 + SMD.esz.sei,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)

#print(publication.bias.model.1.se,digits=3)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.RR.se <- estimates.CI(publication.bias.model.RR.se)

estimates.publication.bias.model.SMD.se <- estimates.CI(publication.bias.model.SMD.se)

```

##### **1.2.1.1 - Modelling "effective sample size" SE (Eq.27)**

According to the meta-regression, there is evidence of small-study effects since the slope of the moderator (i.e. SE) is statistically significant (**slope = `r estimates.publication.bias.model.RR.se[2,2]`, 95% CI = [`r estimates.publication.bias.model.1.se[2,3]`,`r estimates.publication.bias.model.RR.se[2,4]`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.RR.se)[1]*100,1)`%; Figure S1), showing that effect sizes with larger SE are larger, and thus, showing evidence that some small effect sizes with large SE are seemingly missing. Since this meta-regression shows some evidence of small-study bias, we  proceed to run the meta-regression suggested in Equation 28 from the main text, so that we can estimate an overall effect size for the meta-analysis after accouting for the existence of small-study bias.

**For the time being, I have assumed that the slope for SE is statistically significant, which is not in this dataset, but should be for the final chosen example. That is, even the wording it's based on that, i.e. the results for this specific example do not agree with the description of the results. APOLOGIES if it is confussing**

```{r}
publication.bias.model.RR.se.plot <- predict(publication.bias.model.RR.se)

newdat.RR <- data.frame(sei=dataset.mean.diff$RR.esz.sei,
                     fit=publication.bias.model.RR.se.plot$pred,
                     upper=publication.bias.model.RR.se.plot$ci.ub,
                     lower=publication.bias.model.RR.se.plot$ci.lb,
                     stringsAsFactors=FALSE)

#newdat <- unique(newdat[order(newdat$sei),])
newdat.RR <- unique(newdat.RR[order(newdat.RR$sei),])

xaxis <- dataset.mean.diff$RR.esz.sei
yaxis <- dataset.mean.diff$RR

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n"
     #ylim=c(-1,2),
     #xlim=c(0,1.1)
     )


abline(a=0,b=0, lwd=1, lty=1)


axis(1,at=seq(0,1,0.1),
     cex.axis=0.8,tck=-0.02)

axis(2,
     #at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "Effective sample size based standard error (SE)", 
      ylab = "Effect size (lnRR)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=1)

lines(newdat.RR$sei, newdat.RR$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat.RR$sei,rev(newdat.RR$sei)),
        c(newdat.RR$lower,rev(newdat.RR$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S1.** Effect sizes with larger SE are larger, which provides evidence of small-study effects in this meta-analytic dataset. The solid line represents the model estimate and shading shows the 95% confidence intervals.

```{r}
# Application of Equation 28 from the main manuscript
publication.bias.model.RR.sv <- rma.mv(RR, VRR,
                                      mods= ~ 1 + RR.esz.sv,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)

#summary(publication.bias.model.1.sv)

publication.bias.model.SMD.sv <- rma.mv(SMD, VSMD,
                                      mods= ~ 1 + SMD.esz.sv,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.RR.sv <- estimates.CI(publication.bias.model.RR.sv)
```

##### **1.2.1.2 - Modelling SV (Eq.22)**

The follow-up meta-regression including the effect sizes' sampling variances (SV or vi) as the only moderator (see Equation 28 from the main text) provides us a less downwardly  biased estimate of the overall effect after accounting for small-study bias. This estimated overall effect corresponds to the intercept of the meta-regression, which corresponds to **`r estimates.publication.bias.model.RR.se[1,2]` (95% CI = [`r estimates.publication.bias.model.RR.se[1,3]`,`r estimates.publication.bias.model.RR.se[1,4]`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.RR.sv)[1]*100,1)`%).

<br/><br/>

#### **1.2.2 - Time-lag bias test (Eq.23)**

To test for time-lag bias, also called decline effects, we are again going to fit a multilevel meta-regression that has the same random effect structure as the meta-analytic model (Table S1) and that includes the year of publication as the only moderator (see Equation 23 from the main text). The estimated slope for year will tell us whether effect sizes have become smaller over time since the first effect size was published.

```{r}
# Application of Equation 23 from the main manuscript
publication.bias.model.RR.timelag <- rma.mv(RR, VRR,
                                      mods= ~ 1 + year,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)

#summary(publication.bias.model.1.timelag)

# extracting the mean and 95% confidence intervals
estimates.publication.bias.model.RR.timelag <- estimates.CI(publication.bias.model.RR.timelag)

```

The time-lag bias test indeed shows evidence for a decline effects since the slope of year is negative, meaning that effect sizes have trended towards zero over time (**year slope = `r estimates.publication.bias.model.RR.timelag[2,2]`; 95% CI = [`r estimates.publication.bias.model.RR.timelag[2,3]`,`r estimates.publication.bias.model.RR.timelag[2,4]`]**; *R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.RR.timelag)[1]*100,1)`%; Figure S2).

```{r}
publication.bias.model.RR.timelag.plot <- predict(publication.bias.model.RR.timelag,newmods=seq(min(dataset.mean.diff$Year),max(dataset.mean.diff$Year),1))

newdat.RR <- data.frame(year=seq(min(dataset.mean.diff$Year),max(dataset.mean.diff$Year),1),
                     fit=publication.bias.model.RR.timelag.plot$pred,
                     upper=publication.bias.model.RR.timelag.plot$ci.ub,
                     lower=publication.bias.model.RR.timelag.plot$ci.lb,
                     stringsAsFactors=FALSE)


xaxis <- dataset.mean.diff$Year
yaxis <- dataset.mean.diff$RR
cex.study <- 1/dataset.mean.diff$RR.esz.sei

plot(xaxis,yaxis,
     type="n",
     ylab="",
     xlab="",
     xaxt="n",
     yaxt="n",
     ylim=c(-2.25,2.25),
     xlim=c(min(xaxis),max(xaxis))
     )


abline(a=0,b=0, lwd=1, lty=1)


axis(1, at=seq(min(xaxis),max(xaxis),5),
     cex.axis=0.8,tck=-0.02)

axis(2,
     #at=round(seq(-2.5,2.25,0.5),1),
     cex.axis=0.8,las=2,tck=-0.02)

title(xlab = "Year of publication", 
      ylab = "Effect size (lnRR)",
      line = 2.75, cex.lab=1.4)

points(jitter(xaxis,2),yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=cex.study)

lines(newdat.RR$year, newdat.RR$fit, lwd=2.75,col="darkorchid4") 

polygon(c(newdat.RR$year,rev(newdat.RR$year)),
        c(newdat.RR$lower,rev(newdat.RR$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
```

**Figure S2.** The overall published effect size has decreased over time since first described. The solid line represents the model estimate, shading shows the 95% confidence intervals and individual effect sizes are scaled by their precision (1/SE).

<br/><br/>

#### 1.2.3 - **All-in publication bias test (Eq.29)**

When heterogeneity exists, it is best to combine Equations 27 and 23 with other moderators since those additional moderators will generally explain or be predicted to explain some of the heterogeneity among effect sizes. Therefore, in this case we will run a multilevel meta-regression including the effective sample size based standard errors, the year of publication and the following moderators originally included in XXX et al. (XXXX): 'season', 'group composition' and 'type of interactions'.

```{r}
# preparing the moderators that need to be included in a meta-regression that also contains a  moderator with the standard errors of the effect sizes and the year of publication
dataset.r.original$season.num <- ifelse(dataset.r.original$season=="nonbreeding",0,1)
dataset.r.original$int.num <- ifelse(dataset.r.original$interactions=="both",0,1)
dataset.r.original$inttype.num <- ifelse(dataset.r.original$interactiontype=="mix",0,1)

dataset.r.original$season.c <- scale(dataset.r.original$season.num,scale=FALSE)
dataset.r.original$int.c <- scale(dataset.r.original$int.num,scale=FALSE)
dataset.r.original$inttype.c <- scale(dataset.r.original$inttype.num,scale=FALSE)

# Application of Equation 29 from the main manuscript
publication.bias.model.RR.timelag.all.in <- rma.mv(RR, VRR,
                                      mods= ~ 1 + RR.esz.sei + year + Disturbance_category + Biome + `Species category (invader)`,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)

#summary(publication.bias.model.1.timelag.all.in)
publication.bias.model.SMD.timelag.all.in <- rma.mv(SMD, VSMD,
                                      mods= ~ 1 + SMD.esz.sei + year + Disturbance_category + Biome + `Species category (invader)`,
                                      random=list(~ 1 | obsID, ~ 1 | paperID),
                                      method="REML",data=dataset.mean.diff)
```

**Need to add a little text and probably also a table showing the results of this meta-regresssion, which I did not do because it would be very specific for the dataset we will finally used. Nonetheless, find below the *R<sup>2</sup><sub>marginal</sub>* estimate, which for this sparrow example is pretty high and much coming from the publication bias tests**

*R<sup>2</sup><sub>marginal</sub>* = `r round(R2(publication.bias.model.1.timelag.all.in)[1]*100,1)`%


## R Session Information

```{r}
sessionInfo() %>% pander()
```

## References

To be added.